<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>FORLA: Federated Object-centric Representation Learning with Slot Attention</title>
  <meta name="description" content="Learning efficient visual representations across heterogeneous unlabeled datasets in federated learning via slot attention.">
  <!-- Add other meta tags from template (SEO, social preview) -->
  <link rel="stylesheet" href="static/css/styles.css">
</head>
<body>
  <header class="hero">
    <div class="container">
      <h1>FORLA: Federated Object-centric Representation Learning with Slot Attention</h1>
      <p><strong>Guiqiu Liao¹ · Matjaž Jogan¹ · Eric Eaton² · Daniel A. Hashimoto¹ ²</strong><br>
         ¹ PCASO Laboratory, Dept. of Surgery, University of Pennsylvania<br>
         ² Dept. of Computer & Information Science, University of Pennsylvania<br>
         <a href="mailto:guiqiu.liao@pennmedicine.upenn.edu">guiqiu.liao@pennmedicine.upenn.edu</a>
      </p>
      <div class="buttons">
        <a class="btn btn-primary" href="https://arxiv.org/abs/XXXX.XXXXX" target="_blank">arXiv</a>
        <a class="btn btn-outline" href="https://github.com/PCASOlab/FORLA" target="_blank">Code</a>
        <a class="btn btn-outline" href="files/FORLA.pdf" target="_blank">PDF</a>
      </div>
    </div>
  </header>

  <section id="abstract" class="container">
    <h2>Abstract</h2>
    <p>Learning efficient visual representations across heterogeneous unlabeled datasets remains a central challenge in federated learning. Effective federated representations require features that are jointly informative across clients while disentangling client-specific factors without supervision. We introduce <strong>FORLA</strong>, a framework for federated <em>object-centric</em> representation learning and feature adaptation using <em>unsupervised slot attention</em>. A shared feature <em>adapter</em> (trained collaboratively across clients) adapts features from foundation models, and a shared <em>slot attention</em> module reconstructs the adapted features. A two-branch student–teacher design enables a student decoder to reconstruct full features while a teacher reconstructs their low-dimensional adapted counterpart. The shared slot attention aligns <em>object-level</em> representations across clients. Experiments on multiple real-world datasets show that FORLA outperforms centralized baselines on object discovery and learns a compact, universal representation that generalizes across domains.</p>
  </section>

  <section id="method" class="container">
    <h2>Method overview</h2>
    <img src="static/images/forla_stages.jpg" alt="Two-stage training pipeline for FORLA" class="w-full">
    <p class="caption">Figure 1: Two-stage training pipeline. Stage 1: representation distillation & alignment. Stage 2: federated representation rediscovery.</p>
  </section>

  <section id="citation" class="container">
    <h2>Citation</h2>
    <pre><code class="bibtex">
@article{liao2025forla,
  title   = {FORLA: Federated Object-centric Representation Learning with Slot Attention},
  author  = {Liao, Guiqiu and Jogan, Matja{\v z} and Eaton, Eric and Hashimoto, Daniel A.},
  journal = {arXiv preprint arXiv:XXXX.XXXXX},
  year    = {2025}
}
</code></pre>
    <button class="btn btn-copy" data-copy-target=".bibtex">Copy BibTeX</button>
  </section>

  <footer class="container">
    <p>© 2025 PCASO Laboratory, University of Pennsylvania</p>
  </footer>

  <script src="static/js/main.js"></script>
</body>
</html>
